## Notes on the videos
<br />

<details>
<summary>Video: 1 - Introduction to AWS</summary>
<br />

AWS stands for 'Amazon Web Services'. There are many services, but you don't have to know all of them. We are going to use 
- Compute: EC2 (virtual servers in the cloud)
- Storage
- Networking & Content Delivery: VPC (e.g. firewalls)
- Security, Identity & Compliance: IAM
- Containers

### AWS Account and Services Scope
The *global* scope (AWS account, IAM users, Billing, Route53) is divided into *regions* (S3, VPC, DynamoDB), which themselves are divided into *availability zones* (physical datacenters running the virtual machines: e.g. EC2, EBS, RDS).

All AWS services are created in one oth these 3 scopes.

</details>

*****

<details>
<summary>Video: 2 - Create an AWS account</summary>
<br />

Open the [AWS registration page](https://portal.aws.amazon.com/billing/signup), fill in the form and follow the instructions.

</details>

*****

<details>
<summary>Video: 3 - IAM - Manage Users, Roles and Permissions</summary>
<br />

IAM stands for Identity and Access Management. The IAM service lets you manage who has access to your services. You define users or user groups and assign them certain permissions.

When you create an AWS account you have a root user by default with unlimited privileges. So we first should create an admin user with only those privileges needed to create an EC2 instance, deploy applications on it, etc. 

We need an admin user who has privileges to create other users and roles, also system users (like jenkins). Groups can be used to manage the permissions of several users who all have the same permissions.\
If you want to give a service permissions to do something, you cannot directly assign privileges to a service. You have to create a role, assign the privileges to that role, and then assign the role to the service. Each service must have its own role though. You cannot assign the same role to multiple services.

### Creating an Admin User
Open the "Services" dialog (link on the top left), click the filter "All services" and select "Security, Identity & Compliance" > "IAM" > "Access Management" > "Users". Press the "Add users" button and enter a name (e.g. 'admin'). Check the optional "Provide user access to the AWS Management Console" checkbox. Select the radio buttons "I want to create an IAM user" and "Autogenerated password". Check "Users must create a new password at next sign-in" as recommended. Press the "Next" button.

Choose "Attach policies directly" and select the "AdministratorAccess" policy. Press the "Next" button. On the summary page press "Create user". Copy the console sign-in URL, the username and password and save it in your password manager. You may also download a .csv file containing the credentials.

With these credentials the admin user has access to the web console. To provide him also programmatic access from a command line, you need to generate an access key ID and a secret access key. But first log out as root user and login again as the new admin user. Open the console sign-in URL (the account ID should be filled out automatically, otherwise you'll find it in the information you got when creating the AWS account) and enter username and password. On first login you'll have to change the password. Do it and don't forget to update the password in the password manager.

#### Generate an Access key
Go to the users list, click on the new admin user, select the "Security credentials" tab, scroll down to "Access keys" and press the "Create access key" button. Select "Command line interface (CLI)" and check the "I understand the above recommendation and want to proceed to create an access key" checkbox. Press "Next". You may enter a description. Press "Create access key". Copy access key and secret access key, store them in your password manager and download the .csv file. Press "Done".

</details>

*****

<details>
<summary>Video: 4 - Regions and Availability Zones</summary>
<br />

AWS data centers are clustered in 30 regions. Each region has multiple availability zones used for replication. Whenever you create a new service you have to choose a region this service should be allocated in.

</details>

*****

<details>
<summary>Video: 5 - VPC - Manage Private Network on AWS</summary>
<br />

VPC stands for Virtual Private Cloud. Each region has its VPC. Each availability zone is in a subnet of the region's VPC. Each service you are starting has to be running inside a VPC. Subnets are either private (you configured firewall rules that block all the traffic from outside the VPC) or public (you firewall rules allow access from outside the VPC). Inside the VPC your service (e.g. webserver) in a public subnet can access other services (e.g. database) inside a private subnet.

Services may get two IP addresses, a private one for internal communication inside the VPC, and a public one to make it accessible from the internet.

Access can be configured on subnet level, or on service component level. On subnet level so called NACLs (Network Access Control Lists) are used. On instance level it is done in Security Groups. 

</details>

*****

<details>
<summary>Video: 6 - CIDR Blocks explained</summary>
<br />

CIDR stands for Classless Inter-Domain Routing. It specifies a subnet range. 172.31.0.0/16 for example defines an IP range starting from the IP address 172.31.0.0 and ending with 172.31.255.255. The first 16 bits (172.31.) are fixed and the rest can be changed. 

**Links:** 
- [IP Calculator](https://jodies.de/ipcalc?host=10.0.0.0&mask1=16&mask2=)
- [Subnet Calculator](https://mxtoolbox.com/subnetcalculator.aspx)
- [Subnet Divider](https://www.davidc.net/sites/default/subnets/subnets.html)

</details>

*****

<details>
<summary>Video: 7 - Introduction to EC2 Virtual Cloud Server</summary>
<br />

EC2 stands for Elastic Compute Cloud. It is a virtual server providing compute capacity.

As an instructive example, we deploy a web application on an EC2 instance. This includes the following steps:
- Create an EC2 instance on AWS
- Connect to EC2 instance with ssh
- Install Docker on remote EC2 instance
- Run Docker container (docker login, pull, run) from private repository
- Configure EC2 Firewall to access application externally from the browser

### Create an EC2 instance on AWS
Go to "Services" > "Compute" > "EC2". Scroll down to the "Launch instance" section, press the "Launch instance" button and select "Launch instance". This leads you to a page where you can configure the new instance.

Enter a name (e.g. web-server) and the "Add additional tags" link. Press the "Add tag" button and enter the key-value-pair "Type" -> "web-server-with-docker".

Scroll down to "Application and OS Images (Amazon Machine Image)" and select the machine image "Amazon Linux". Next you can select from a large list of instance types. Select the free tier eligible "t2.micro".

To be able to ssh into the EC2 server we have to generate a key pair. We don't to it on our local machine and copy the public key to the server, but create the key pair on AWS and download the private key. The public key is automatically stored in the right place. Key pairs can be shared among different EC2 instances. Scroll down to "Key pair (login)" and click the "Create new key pair" link. Enter the name 'docker-server', select "RSA" and ".pem" and press the "Create key pair" button. The `docker-server.pem` file holding the private key is automatically downloaded.

In the "Network settings" section we could choose a VPC and a subnet (availability zone), but we leave the defaults unchanged. Make sure "Auto-assign public IP" is enabled. Select "Create security group", change the security group name to 'security-group-docker-server' and the description accordingly, leave the ssh firewall rule unchanged but modify the source type from "Anywhere" to "My IP".

Leave the defaults in the "Configure storage section" unchanged.

In the summary column on the right you could change the number of instances to be created, but we leave it at the default value of 1. Press the "Launch instance" button.

### Connect to EC2 Instance
Move the downloaded 'docker-server.pem' file into the ssh folder `~/.ssh` and restrict the file permissions 'read for you only': `chmod 400 ~/.ssh/docker-server.pem`.

Open the AWS web console, go to "EC2 Dashboard" > "Instances" and check the 'web-server' instance. Select the "Networking" tab below and copy the public IPv4 address.

Now open a terminal on your local machine and ssh into the EC2 server as 'ec2-user':\
`ssh -i ~/.ssh/docker-server.pem ec2-user@<public-ip-address>`.

### Install Docker on EC2
Execute the following commands on the EC2 terminal:
```sh
sudo yum update
sudo yum install docker
sudo service docker start
# add the ec2-user to the docker group 
# to avoid having to use sudo for every docker command
sudo usermod -aG docker ec2-user
# the last command will be effective only after a re-login
exit
```

### Run Webapplication on EC2
Go to the react-nodejs-example application (in the sample-applications folder or clone it from [GitHub](https://github.com/nanuchi/react-nodejs-example)) and build a Docker image, login to DockerHub and push the image to your private Docker registry:
```sh
docker build -t fsiegrist/fesi-repo:devops-bootcamp-react-nodjs-1.0 .
docker login
docker push fsiegrist/fesi-repo:devops-bootcamp-react-nodjs-1.0
```

*****
**Personal Note**
 Because my local machine has an Apple M2 processor (arm64) and the EC2 virtual machine we just created has an amd64 processor, the usual 'docker build' command would create an image runnable on arm64 only. So I would either have to slightly modify the Dockerfile and replace 'FROM node:10' with 'FROM --platform=linux/amd64 node:10' or I can use [docker buildx](https://docs.docker.com/engine/reference/commandline/buildx/) to build images for specific platforms:
```sh
docker buildx create --use
docker login
docker buildx build --platform linux/amd64,linux/arm64 -t fsiegrist/fesi-repo:devops-bootcamp-react-nodjs-1.0 --push .
```
*****

Now switch back to the EC2 terminal, login to DockerHub, pull the image and start a container from it:
```sh
ssh -i ~/.ssh/docker-server.pem ec2-user@<public-ip-address>

docker login
docker pull --platform linux/amd64 fsiegrist/fesi-repo:devops-bootcamp-react-nodjs-1.0
docker run -d -p 3000:3080 fsiegrist/fesi-repo:devops-bootcamp-react-nodjs-1.0
```

### Make App accessible from the Browser
Open the AWS web console, go to "EC2 Dashboard" > "Instances" and check the 'web-server' instance. Select the "Security" tab below and click on the link for the 'security-group-docker-server'. Open the "Inbound rules" tab and press the "Edit inbound rules" button. Press "Add rule" and enter a rule of type "Custom TCP" for port 3000 with source "Anywhere IPv4". Press "Save rules".

Now open the browser and navigate to `http://<ec2-public-ip>:3000` to see the application in action.

</details>

*****

<details>
<summary>Video: 8, 9, 10 - Deploy to EC2 Server from Jenkins Pipeline</summary>
<br />

## Deploy an Application by Manually Starting a Docker Container
After having built a Docker image containing our application and pushed it to a Docker repository, we are ready to deploy it on a server. In the deploy stage of the Jenkins pipeline we ssh into an EC2 server and execute a docker run command to pull the image and start a container running the application. To be able to do that, we have to install an SSH agent plugin and create according credentials.

### Install SSH Agent Plugin and Create SSH Credentials
Login to the Jenkins management web console and install the "SSH Agent" plugin. Then open the multibranch pipeline ("Dashboard" > "devops-bootcamp-multibranch-pipeline"), open the pipeline specific "Credentials", scroll down to "Stores scoped to devops-bootcamp-multibranch-pipeline" and click on the devops-bootcamp-multibranch-pipeline link and then on the "Global credentials (unrestricted)" link. Press the "Add credentials" button, select the kind "SSH Username with private key", enter an ID (e.g. ec2-server-key), the username 'ec2-user', select "Private Key" > "Enter directly", press the "Add" button and paste the content of the `~/.ssh/docker-server.pem` file you downloaded from the EC2 server. (To copy the content on a mac without having to display it on the terminal, use `pbcopy < ~/.ssh/docker-server.pem`.) Press the "Create" button.

### Add Deploy Stage to Jenkinsfile
To find out how to use the SSH Agent plugin in a Jenkinsfile, we go back to the multibranch pipeline project and click on the item "Pipeline Syntax" in the left menu. Select "sshagent: SSH Agent" in the Sample Step dropdown. The "ec2-user" is already selected (since it is the only SSH credentials username we have). Press the "Generate Pipeline Script" button and copy the example snippet.

Now open the Jenkinsfile in the application project, which is built in the multibranch pipeline (java-maven-app) and add the following stage:
```groovy
stage('Deploy Application') {
    steps {
        script {
            echo 'deploying Docker image to EC2 server...'
            def dockerCmd = "docker run -d -p 8000:8080 fsiegrist/fesi-repo:devops-bootcamp-java-maven-app-${IMAGE_TAG}"
            sshagent(['ec2-server-key']) {
                sh "ssh -o StrictHostKeyChecking=no ec2-user@<ec2-public-ip> ${dockerCmd}"
            }
        }
    }
}
```

The option `-o StrictHostKeyChecking=no` is necessary to avoid ssh asking whether the server should be added to the known hosts.

### Comfigure EC2
To make this work, two more things have to be done on the EC2 server:
- To allow Jenkins to ssh into the EC2 server, we have to add the IP address of the Jenkins host (droplet) to the firewall rule restricting access via port 22.
- To allow EC2 to pull a Docker image from our private repository on DockerHub, we have to login from EC2 to DockerHub once. This will create an entry in `/home/ec2-user/.docker/config.json` and keep the ec2-user logged in.

And to allow accessing the application from the internet, we have to add a firewall rule opening the port 8000 from anywhere.

## Use Docker Compose for Deployment
Usually applications do not consist of just one Docker container. As soon as multiple containers have to be managed it is easier to do that using Docker Compose. So instead of executing `docker run` commands on the deployment server, a `docker-compose.yaml` file is part of the application project (in the Git repository), copied to the deployment server and executed using Docker Compose. This section shows how to do this from a Jenkins pipeline.

### Install Docker Compose on EC2
Download Docker Compose:\
`sudo curl -SL https://github.com/docker/compose/releases/download/v2.17.2/docker-compose-linux-x86_64 -o /usr/local/bin/docker-compose`

Make the docker-compose command executable:\
`sudo chmod +x /usr/local/bin/docker-compose`

Test the installation:\
`docker-compose --version`

### Create a Docker Compose File
Add a file called `docker-compose.yaml` with the following content to the 'java-maven-app' project:
```yaml
version: '3.9'
services:
  java-maven-app:
    image: fsiegrist/fesi-repo:devops-bootcamp-java-maven-app-${IMAGE_TAG}
    ports:
      - 8000:8080

  postgres:
    image: postgres:13
    ports:
      - 5432:5432
    environment:
      - POSTGRES_PASSWORD:my-pwd
```
The second service (container) postgres is added just for demonstration purposes.

### Adjust Jenkinsfile
Adjust the deploy stage of the application's Jenkinsfile to the following content:
```groovy
stage('Deploy Application') {
    steps {
        script {
            echo 'deploying Docker image to EC2 server...'
            def dockerComposeCmd = "IMAGE_TAG=${IMAGE_TAG} docker-compose -f docker-compose.yaml up -d"
            sshagent(['ec2-server-key']) {
                sh 'scp -o StrictHostKeyChecking=no docker-compose.yaml ec2-user@<ec2-public-ip>:/home/ec2-user'
                sh "ssh -o StrictHostKeyChecking=no ec2-user@35.156.226.244 ${dockerComposeCmd}"
            }
        }
    }
}
```
Commit and push the changes to the Git repository and start the build pipeline on Jenkins.

### Extract the Logic to a Shell Script
Add a shell script called `server-cmds.sh` with the following content to the application project:
```sh
#!/usr/bin/env/ bash

export IMAGE_TAG=$1
docker-compose -f docker-compose.yaml up -d
echo "successfully started the containers using docker-compose"
```

Adjust the deploy stage of the application's Jenkinsfile to the following content:
```groovy
stage('Deploy Application') {
    steps {
        script {
            echo 'deploying Docker image to EC2 server...'
            def shellCmd = "bash ./server-cmds.sh ${IMAGE_TAG}"
            sshagent(['ec2-server-key']) {
                sh 'scp -o StrictHostKeyChecking=no server-cmds.sh docker-compose.yaml ec2-user@<ec2-public-ip>:/home/ec2-user'
                sh "ssh -o StrictHostKeyChecking=no ec2-user@<ec2-public-ip> ${shellCmd}"
            }
        }
    }
}
```
Commit and push the changes to the Git repository and start the build pipeline on Jenkins.

</details>

*****
